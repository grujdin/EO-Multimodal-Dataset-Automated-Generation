import os
import json
import urllib.parse
from io import BytesIO

import streamlit as st
import pandas as pd
import requests
import folium
from rdflib import Graph, Namespace
from rdflib.namespace import RDF, RDFS
from streamlit_folium import st_folium
from pandas.errors import EmptyDataError
from gdacs.api import EVENT_TYPES

# --------------------------------------------------
# Constants & Namespaces
# --------------------------------------------------
DIS = Namespace("http://example.org/disaster#")
SUMMARY_DIR = os.path.join(os.getcwd(), "summary")
DETAILS_DIR = "gdacs_event_details"
GEOM_DIR    = "gdacs_event_geometry"

# --------------------------------------------------
# Helpers
# --------------------------------------------------
@st.cache_data(show_spinner=False)
def load_hazard_ontology(file_obj):
    g = Graph()
    g.parse(file_obj)
    out = []
    for subj in g.subjects(RDF.type, DIS.GDACSHazardType):
        lbl  = g.value(subj, RDFS.label)
        code = g.value(subj, DIS.gdacsCode)
        out.append({
            "label": str(lbl)  if lbl  else None,
            "code":  str(code) if code else None
        })
    return sorted(out, key=lambda h: (h["label"] or "").lower())

def first_lon_lat(geom):
    t = geom.get("type","")
    c = geom.get("coordinates",[])
    try:
        if t=="Point":
            return c[0], c[1]
        if t=="Polygon":
            return c[0][0][0], c[0][0][1]
        if t=="MultiPolygon":
            return c[0][0][0][0], c[0][0][0][1]
    except:
        pass
    return 0.0, 0.0

def append_new_csv(df_new: pd.DataFrame, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    if df_new.empty:
        return
    if not os.path.exists(path) or os.path.getsize(path)==0:
        df_new.to_csv(path, index=False)
        return
    try:
        df_old = pd.read_csv(path)
    except (EmptyDataError, pd.errors.ParserError):
        df_new.to_csv(path, index=False)
        return
    if not {"event_type","event_id"}.issubset(df_old.columns):
        df_new.to_csv(path, index=False)
        return
    existing = set(zip(df_old["event_type"], df_old["event_id"]))
    mask = [(et,eid) not in existing for et,eid in zip(df_new["event_type"],df_new["event_id"])]
    df_add = df_new[mask]
    if df_add.empty:
        return
    pd.concat([df_old, df_add], ignore_index=True).to_csv(path, index=False)

# --------------------------------------------------
# Page & Sidebar
# --------------------------------------------------
st.set_page_config(page_title="GDACS Semantic Connector", layout="wide")
st.sidebar.title("üåê GDACS Semantic Query")

owl_file = st.sidebar.file_uploader(
    "Upload Hazard Ontology (.owl)",
    type=["owl"],
    key="owl_upload"
)
if not owl_file:
    st.sidebar.info("Please upload an OWL file to proceed.")
    st.stop()

hazards   = load_hazard_ontology(owl_file)
supported = [c for c in EVENT_TYPES if c]
valid     = [h for h in hazards    if h["code"] in supported]
invalid   = [h for h in hazards    if h["code"] not in supported]

if invalid:
    st.sidebar.warning(
        "Skipped (no GDACS code): " + ", ".join(h["label"] for h in invalid if h["label"])
    )
if not valid:
    st.sidebar.error("No valid hazards found in your ontology.")
    st.stop()

labels    = [h["label"] for h in valid]
sel_label = st.sidebar.selectbox("Select Hazard Type", labels, key="haz")
sel_code  = next(h["code"] for h in valid if h["label"]==sel_label)
limit     = st.sidebar.selectbox("Limit number of results",
                                 [10,20,50,100,200,500,1000],
                                 index=0, key="lim")

# --------------------------------------------------
# Title & Summary Fetch
# --------------------------------------------------
st.title("üåç GDACS Semantic Connector")
st.caption("Unofficial GDACS API explorer with ontology-driven hazard selection, detail & episode footprints.")

st.header("üóÇÔ∏è Summary Events")
st.markdown("Using <https://www.gdacs.org/observatory/api/data> (unofficial)")

if st.sidebar.button("üîç Fetch GDACS Events", key="fetch_sum"):
    from gdacs.api import GDACSAPIReader
    client = GDACSAPIReader()
    with st.spinner(f"Querying {sel_label} events‚Ä¶"):
        try:
            gj = client.latest_events(event_type=sel_code, limit=limit)
        except Exception as e:
            st.error(f"Failed to fetch: {e}")
            gj = None

    rows = []
    if gj and getattr(gj, "features", []):
        for feat in gj.features:
            p = feat.get("properties",{}) or {}
            g = feat.get("geometry",{})   or {}
            lon,lat = (None,None)
            if "coordinates" in g:
                lon,lat = g["coordinates"][:2]
            et,eid = p.get("eventtype"), p.get("eventid")
            rows.append({
                "event_type":   et,
                "event_id":     eid,
                "name":         p.get("name"),
                "alert":        p.get("alertlevel"),
                "from_date":    p.get("fromdate"),
                "to_date":      p.get("todate"),
                "country":      p.get("country"),
                "severity":     p.get("severitydata",{}).get("severity"),
                "latitude":     lat,
                "longitude":    lon,
                "report_url":   p.get("url",{}).get("report"),
                "geometry_url": p.get("url",{}).get("geometry"),
                "detail_url":   f"https://www.gdacs.org/gdacsapi/api/events/geteventdata?eventtype={et}&eventid={eid}"
            })
    else:
        st.warning("‚ö†Ô∏è No events returned.")
    st.session_state["summary_df"] = pd.DataFrame(rows)

# --------------------------------------------------
# Display & Persist Summary
# --------------------------------------------------
if "summary_df" in st.session_state:
    df = st.session_state["summary_df"]
    st.dataframe(df, use_container_width=True)

    os.makedirs(SUMMARY_DIR, exist_ok=True)
    csv_p = os.path.join(SUMMARY_DIR, "gdacs_summary.csv")
    append_new_csv(df, csv_p)

    with open(csv_p,"rb") as f:
        st.download_button("üì• Download Summary CSV",
                           data=f.read(),
                           file_name="gdacs_summary.csv",
                           mime="text/csv",
                           key="dl_csv")

    df_csv = pd.read_csv(csv_p)
    buf = BytesIO()
    with pd.ExcelWriter(buf, engine="openpyxl") as wr:
        df_csv.to_excel(wr, index=False)
    buf.seek(0)
    st.download_button("‚¨áÔ∏è Download Summary Excel",
                       data=buf.getvalue(),
                       file_name="gdacs_summary.xlsx",
                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                       key="dl_xlsx")

# --------------------------------------------------
# ‚ùóÔ∏è NEW: Episode-Level Details Fetch & Save
# --------------------------------------------------
if "summary_df" in st.session_state:
    st.markdown("---")
    st.subheader("üìã GDACS Episode Details")

    if st.button("üì• Fetch Details for Each Event", key="fetch_det"):
        os.makedirs(DETAILS_DIR, exist_ok=True)
        detail_files = []   # ‚Üê collect only this run‚Äôs files

        for _,row in st.session_state["summary_df"].iterrows():
            et, eid = row["event_type"], row["event_id"]
            evd = os.path.join(DETAILS_DIR, f"{et}_{eid}")
            os.makedirs(evd, exist_ok=True)

            # get episodes list
            try:
                r = requests.get(row["detail_url"]); r.raise_for_status()
                props = r.json().get("properties",{}) or {}
            except Exception as e:
                st.error(f"{et} {eid}: failed to fetch event JSON: {e}")
                continue

            for ep in props.get("episodes", []):
                det_url = ep.get("details","")
                qs      = urllib.parse.parse_qs(urllib.parse.urlparse(det_url).query)
                epid    = qs.get("episodeid",[None])[0]
                if not epid:
                    continue

                fname = os.path.join(evd, f"episode_{epid}.xlsx")
                # only fetch + save if missing
                if not os.path.exists(fname):
                    try:
                        r2 = requests.get(det_url); r2.raise_for_status()
                        js = r2.json()
                        df_det = pd.json_normalize(js)
                        df_det.to_excel(fname, index=False)
                        st.success(f"Saved ‚Üí {et}_{eid}/episode_{epid}.xlsx")
                    except Exception as e:
                        st.warning(f"{et} {eid} ep {epid}: failed detail fetch: {e}")
                        continue

                detail_files.append((f"{et}_{eid}", f"episode_{epid}.xlsx", fname))

        # remember only this run‚Äôs files
        st.session_state["detail_files"] = detail_files

    # download links
    if st.session_state.get("detail_files"):
        st.markdown("### üìé Download Episode Detail Files")
        for ev_sub, fn, fp in st.session_state["detail_files"]:
            label = f"{ev_sub}/{fn}"
            with open(fp, "rb") as f:
                st.download_button(
                    label=label,
                    data=f.read(),
                    file_name=f"{ev_sub}_{fn}",
                    key=f"dl_det_{ev_sub}_{fn}"
                )

# --------------------------------------------------
# Episode-Level Geometry Fetch & Display
# --------------------------------------------------
if "summary_df" in st.session_state:
    st.markdown("---")
    st.subheader("‚åõ Fetch & Save Episode Footprints")

    if st.button("üåê Fetch Episodes & Geometry", key="fetch_geo"):
        os.makedirs(GEOM_DIR, exist_ok=True)
        evos = {}
        for _,row in st.session_state["summary_df"].iterrows():
            et,eid,name = row["event_type"], row["event_id"], row["name"]
            evg = os.path.join(GEOM_DIR, f"{et}_{eid}")
            os.makedirs(evg, exist_ok=True)

            # reuse episodes list from detail URL
            try:
                r = requests.get(row["detail_url"]); r.raise_for_status()
                eps = r.json().get("properties",{}).get("episodes",[])
            except:
                eps = []

            fps = []
            for ep in eps:
                det_url = ep.get("details","")
                qs = urllib.parse.parse_qs(urllib.parse.urlparse(det_url).query)
                epid = qs.get("episodeid",[None])[0]
                if not epid:
                    continue

                geom_url = (
                    f"https://www.gdacs.org/gdacsapi/api/polygons/getgeometry"
                    f"?eventtype={et}&eventid={eid}&episodeid={epid}"
                )
                try:
                    r2 = requests.get(geom_url); r2.raise_for_status()
                    gj = r2.json()
                except:
                    continue

                fn = os.path.join(evg, f"episode_{epid}.geojson")
                with open(fn,"w",encoding="utf-8") as f:
                    json.dump(gj, f)

                # pick timestamp
                poly = next((f for f in gj["features"]
                             if f["geometry"]["type"] in ("Polygon","MultiPolygon")), None)
                ts = poly["properties"].get("polygondate") if poly else "N/A"
                fps.append((epid, ts, fn))

            evos[f"{et}_{eid}"] = fps
        st.success("All episode geometries fetched & saved.")
        st.session_state["evolution"] = evos

    if st.session_state.get("evolution"):
        st.markdown("---")
        st.header("üìà Evolution of Event Footprints")
        for evkey, fps in st.session_state["evolution"].items():
            st.subheader(evkey.replace("_"," "))
            for epid, ts, path in fps:
                chk = st.checkbox(f"Episode {epid} ‚Äî {ts}", key=f"chk_{evkey}_{epid}")
                if not chk:
                    continue
                gj = json.load(open(path,"r",encoding="utf-8"))
                pt = next((f for f in gj["features"]
                           if f["geometry"]["type"]=="Point"), None)
                if pt:
                    lon0, lat0 = pt["geometry"]["coordinates"][:2]
                else:
                    lon0, lat0 = first_lon_lat(gj["features"][0]["geometry"])

                m = folium.Map(location=[lat0,lon0], zoom_start=6, tiles="CartoDB positron")
                folium.GeoJson(
                    gj,
                    style_function=lambda f: {
                        "fillColor":"#228B22","color":"#222","weight":1,"fillOpacity":0.6
                    }
                ).add_to(m)
                folium.CircleMarker(
                    location=(lat0,lon0),
                    radius=5, color="crimson",
                    fill=True, fill_color="crimson", fill_opacity=0.9
                ).add_to(m)
                st_folium(m, width=700, height=400)

                with open(path,"rb") as f:
                    st.download_button(
                        f"‚¨áÔ∏è Download GeoJSON Episode {epid}",
                        data=f.read(),
                        file_name=os.path.basename(path),
                        key=f"dl_geo_{evkey}_{epid}"
                    )
            st.markdown("‚Äî"*20)

# --------------------------------------------------
# Footer
# --------------------------------------------------
st.markdown(
    "> **What do these polygons represent?**  \n"
    "> GDACS publishes an *event-footprint* layer derived from satellite and model analysis.  "
    "> ‚Ä¢ **Flood (FL)** polygons outline the estimated inundated area.  "
    "> ‚Ä¢ **Wildfire (WF)** polygons show the hot-spot / burned-area perimeter.  "
    "> ‚Ä¢ For earthquakes, polygons approximate the affected region based on shaking intensity.  "
    "> They are **not** administrative boundaries but hazard-specific footprints generated by GDACS."
)
